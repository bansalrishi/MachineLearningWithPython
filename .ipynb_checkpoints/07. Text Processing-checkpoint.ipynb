{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topics\n",
    "#### Task\n",
    "\n",
    "1. Tokenization  \n",
    "2. Stopword Removal  \n",
    "3. N- Grams  \n",
    "4. Stemming  \n",
    "5. Word Sense Disambiguation  \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Tokenization\n",
    "Taking a text or set of text and breaking it up into its individual words\n",
    "<img src=\"Image\\token.JPG\" width=300>\n",
    "\n",
    "- Word Tokenization\n",
    "- Sentence Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['You are ready to learn and do your best.', 'but you are also nervous.']\n",
      "['You', 'are', 'ready', 'to', 'learn', 'and', 'do', 'your', 'best', '.', 'but', 'you', 'are', 'also', 'nervous', '.']\n",
      "[['You', 'are', 'ready', 'to', 'learn', 'and', 'do', 'your', 'best', '.'], ['but', 'you', 'are', 'also', 'nervous', '.']]\n"
     ]
    }
   ],
   "source": [
    "#Tokenization\n",
    "import nltk\n",
    "#nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "text = \"You are ready to learn and do your best. but you are also nervous.\"\n",
    "sents = (sent_tokenize(text)) \n",
    "print(sents)\n",
    "print(word_tokenize(text))\n",
    "words = [word_tokenize(sent) for sent in sents]\n",
    "print(words)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Stopword Removal\n",
    "A stop word is a commonly used word (such as “the”, “a”, “an”, “in”) that a search engine has been programmed to ignore.\n",
    "<img src=\"Image\\stop.jpg\" width=400>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['You', 'ready', 'learn', 'best', 'also', 'nervous']\n"
     ]
    }
   ],
   "source": [
    "#Removing Stopwords\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "text = \"You are ready to learn and do your best. but you are also nervous.\"\n",
    "#make set of stopword and punctuation\n",
    "customstopwords=set(stopwords.words('english')+list(punctuation))\n",
    "wordslist=[word for word in word_tokenize(text) if word not in customstopwords]\n",
    "print(wordslist)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. N-Grams\n",
    "An n-gram is a contiguous sequence of n items from a given sample of text or speech.\n",
    "\n",
    "<img src=\"Image\\n-grams.jpg\" width=300>\n",
    "\n",
    "- While typing we get suggestion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('You', 'ready'), 1),\n",
       " (('also', 'nervous'), 1),\n",
       " (('best', 'also'), 1),\n",
       " (('learn', 'best'), 1),\n",
       " (('ready', 'learn'), 1)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#N-grams\n",
    "import nltk\n",
    "from nltk.collocations import *\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "#trigram_measures = nltk.collocations.TrigramAssocMeasures()\n",
    "finder = BigramCollocationFinder.from_words(wordslist)\n",
    "#most important bigram on top\n",
    "sorted(finder.ngram_fd.items())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Stemming\n",
    "Stemming is the process of reducing inflected (or sometimes derived) words to their word stem, base or root form\n",
    "\n",
    "<img src=\"Image\\stem.jpg\" width=300>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['it', 'is', 'import', 'to', 'by', 'very', 'python', 'whil', 'you', 'ar', 'python', 'with', 'python', '.', 'al', 'python', 'hav', 'python', 'poor', 'at', 'least', 'ont', '.']\n"
     ]
    }
   ],
   "source": [
    "#Stemming\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "st = LancasterStemmer()\n",
    "new_text = \"It is important to by very pythonly while you are pythoning with python. All pythoners have pythoned poorly at least once.\"\n",
    "stemmedwords=[st.stem(word) for word in word_tokenize(new_text)]\n",
    "print(stemmedwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('It', 'PRP'),\n",
       " ('is', 'VBZ'),\n",
       " ('important', 'JJ'),\n",
       " ('to', 'TO'),\n",
       " ('by', 'IN'),\n",
       " ('very', 'RB'),\n",
       " ('pythonly', 'RB'),\n",
       " ('while', 'IN'),\n",
       " ('you', 'PRP'),\n",
       " ('are', 'VBP'),\n",
       " ('pythoning', 'VBG'),\n",
       " ('with', 'IN'),\n",
       " ('python', 'NN'),\n",
       " ('.', '.'),\n",
       " ('All', 'DT'),\n",
       " ('pythoners', 'NNS'),\n",
       " ('have', 'VBP'),\n",
       " ('pythoned', 'VBN'),\n",
       " ('poorly', 'RB'),\n",
       " ('at', 'IN'),\n",
       " ('least', 'JJS'),\n",
       " ('once', 'RB'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Part of Speech\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "nltk.pos_tag(word_tokenize(new_text))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Word Sense Disambiguation\n",
    "WSD is identifying which sense of a word (i.e. meaning) is used in a sentence, when the word has multiple meanings. \n",
    "<img src=\"Image\\wordsense.jpg\" width=300>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('mouse.n.01') any of numerous small rodents typically resembling diminutive rats having pointed snouts and small ears on elongated bodies with slender usually hairless tails\n",
      "Synset('shiner.n.01') a swollen bruise caused by a blow to the eye\n",
      "Synset('mouse.n.03') person who is quiet or timid\n",
      "Synset('mouse.n.04') a hand-operated electronic device that controls the coordinates of a cursor on your computer screen as you move it around on a pad; on the bottom of the device is a ball that rolls on the surface of the pad\n",
      "Synset('sneak.v.01') to go stealthily or furtively\n",
      "Synset('mouse.v.02') manipulate the mouse of a computer\n"
     ]
    }
   ],
   "source": [
    "#Word Sense Disambiguation\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "#nltk.download('wordnet')\n",
    "for ss in wn.synsets('mouse'):\n",
    "    print (ss, ss.definition())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('bass.n.07') the member with the lowest range of a family of musical instruments\n",
      "Synset('sea_bass.n.01') the lean flesh of a saltwater fish of the family Serranidae\n",
      "Synset('mouse.v.02') manipulate the mouse of a computer\n"
     ]
    }
   ],
   "source": [
    "from nltk.wsd import lesk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "sense1 = lesk(word_tokenize(\"Sing in a lower tone, along with the bass\"), 'bass')\n",
    "print (sense1, sense1.definition())\n",
    "\n",
    "sense2 = lesk(word_tokenize(\"The sea bass really very hard to catch\"), 'bass')\n",
    "print (sense2, sense2.definition())\n",
    "\n",
    "sense3 = lesk(word_tokenize(\"Cat is chasing the mouse\"), 'mouse')\n",
    "print (sense3, sense3.definition())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
